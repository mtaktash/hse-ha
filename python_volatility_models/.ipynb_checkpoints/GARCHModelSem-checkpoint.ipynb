{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценивание моделей типа ARCH \n",
    "\n",
    "Одномерные ARCH модели появились в литературе с работой Engle (1982), и были вскоре обобщены в модель GARCH в работе Bollerslev (1986). Изначально их использовали для предсказания волатильности ряда инфляции, однако модели оказались релевантны и для предсказания волатильности финансовой доходности, наблюдаемой на месячных и более частых данных. Это позволяет изучать межвременное соотношение между риском и ожидаемой доходность.\n",
    "\n",
    "Документация к пакету ARCH\n",
    "http://arch.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.mpl.rcParams['figure.figsize'] = (15, 5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "## Загрузка данных\n",
    "Загружаем данные о цене акций напрямую с Yahoo Finance при помощи пакета `pandas_datareader`. \n",
    "Пример для акций Яндекса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as pdr\n",
    "import fix_yahoo_finance \n",
    "fix_yahoo_finance.pdr_override() # Yahoo changed their API some days ago, it's a temporary fix \n",
    "\n",
    "# download dataframe\n",
    "start = dt.datetime(2011,1,1)\n",
    "end = dt.datetime(2018,1,1)\n",
    "data = pdr.get_data_yahoo('YNDX', start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выберите компанию для анализа, загрузите данные о ее цене\n",
    "# Постройте график цены акции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стилизованные факты о ряде доходности\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $P_t, t = 0, 1, ..., n$ — цена финансового актива в момент времени $t$. Тогда доходность актива определяется как\n",
    "\n",
    "$$ y_t = \\dfrac{P_t - P_{t-1}}{P_{t-1}} \\approx \\ln P_t - \\ln P_{t-1} $$\n",
    "\n",
    "Ряд $y_t$ отражает типичные «стилизованные факты», присущие финансовым данным:\n",
    "\n",
    "- Volatility clustering — за большими изменениями доходности следуют большие изменения, за малыми — малые (видно из графиков $y_t, y_t^2$ и положительной автокорреляции $y_t^2$)\n",
    "- Fat tails — экстремальные значения появляются чаще, чем при нормальном распределении (QQ-plot $y_t$ и гистограмма)\n",
    "- Кроме того, в данных присутствует leverage effect (не отражено на графиках) — положительные и отрицательные изменения доходности по разному влияют на волатильность\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчитайте returns для выбранного актива и returns^2\n",
    "# Посчитайте количество наблюдений, среднее значение, стандартное отклонение \n",
    "# и другие описательные статистики для ряда цен актива"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Постройте график доходности, доходности^2, автокорреляционные функции доходности и доходности^2\n",
    "# Постройте гистограмму распределения доходности и сравните ее с нормальным распределением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То, что распределение доходности $y_t$ не соответствует нормальному распределению, подтверждается тестом Jarque-Bera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверьте это"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структура модели GARCH\n",
    "\n",
    "Мы определяем модель GARCH для $y_t$ (доходность финансового актива) следующим образом:\n",
    "\n",
    "$$ y_t - \\mu_t = \\epsilon_t = \\sigma_t z_t $$\n",
    "\n",
    "где \n",
    "1. $z_t, t=0,1, ...$ — ненаблюдаемые i.i.d. случайные величины с $\\mathbb{E}z_t = 0, \\mathbb{V}ar (z_t) = 1$\n",
    "\n",
    "2. $\\mu_t, \\sigma_t$ — условное математическое ожидание и дисперсия $y_t$\n",
    "\n",
    "\n",
    "### GARCH(p,q)\n",
    "\n",
    "Наиболее часто используемая спецификация GARCH предполагает, что наилучший прогноз дисперсии будущего периода — это взвешенное значение средней дисперсии в LR (константа $\\omega$), дисперсии, предсказанной для текущего периода, с учетом $q$ лагов ($\\sum_{i=1}^q \\beta_i \\sigma^2_{t-i}$) и новой информации, поступившей в предыдущие $p$ периодов (most recent squared residual or so-called innovation series — $\\sum_{i=1}^p \\alpha_i \\epsilon^2_{t-i}$)\n",
    "\n",
    "$$ \\sigma^2_t = \\omega + \\sum_{i=1}^p \\alpha_i \\epsilon^2_{t-i} + \\sum_{i=1}^q \\beta_i \\sigma^2_{t-i} $$\n",
    "\n",
    "\n",
    "Спецификация GARCH(p,q) достаточно хорошо описывает volatility clustering и fat tails (при правильном подборе распределения ошибок), но не отражает leverage effect (разное влияние положительных и отрицательных изменений доходности). Поэтому было создано большое количество моделей, улавливающих эту асимметрию. Например, EGARCH, GJR-GARCH, TARCH, и т.д. \n",
    "\n",
    "\n",
    "### In financial applications:\n",
    "The dependent variable is the return on an asset or portfolio and the variance of the return represents the risk level of those returns. \n",
    "\n",
    "Many banks and other financial institutions use the concept of “Value at Risk” as a way to measure the risks faced by their portfolios. The 1% Value at Risk is defined as the number of dollars that one can be 99 percent certain exceeds any losses for the next day. Statisticians call this a 1% quantile because 1% of the outcomes are worse and 99% are better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценивание различных моделей ARCH \n",
    "\n",
    "http://arch.readthedocs.io/en/latest/univariate/univariate_volatility_modeling.html\n",
    "\n",
    "\n",
    "## GARCH (with a constant mean)\n",
    "\n",
    "The simplest way to specify a model is to use the model constructor arch.arch_model which can specify most common models. The simplest invocation of arch will return a model with a constant mean, GARCH(1,1) volatility process and normally distributed errors.\n",
    "### 1. Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "# Оцените модель GARCH(1,1) при помощи пакета arch\n",
    "# Посмотрите на таблицу с результатами оценивания модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually the GARCH coefficient $\\beta_1$ is found to be around 0.9 for many daily or weekly financial time series — so our model is capable of explaining volatility clustering.\n",
    "\n",
    "- We can show, that the closer $\\alpha_1 + \\beta_1$ is to one, the longer is the influence of a volatility shock. (We know that the volatility \"mean reverts\" to its long run level)\n",
    "\n",
    "- If a1 + b1 > 1, the GARCH model is non-stationary and the volatility will eventually explode to infinity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрите на коеффициенты в вашей модели, выполняются ли вышеописанные факты?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model evaluation\n",
    "After a GARCH model has been fit to the data, the adequacy of the fit can be evaluated using a number of graphical and statistical diagnostics. If the GARCH model is correctly specified, then the estimated standardized residuals should behave like classical regression residuals; i.e., they should not display serial correlation, conditional heteroskedasticity or any type of nonlinear dependence. In addition, the distribution of the standardized residuals  should match the specified error distribution used in the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Постройте график белого шума"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be estimated using the first 7 years to estimate parameters and then forecasts will be produced for the final year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оцените модель по первым нескольким годам в ваших данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly visualize the standardized residuals and conditional volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Постройте график стандартизированных остатков и условной волатильности,\n",
    "# сравните распределение остатков с белым шумом\n",
    "# hint: нужные графики можно построить с помощью пакета arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any call to forecast() returns a ARCHModelForecast object with has 3 core attributes:\n",
    "\n",
    "- mean - The forecast conditional mean.\n",
    "- variance - The forecast conditional variance.\n",
    "- residual_variance - The forecast conditional variance of residuals. This will differ from variance whenever the model has dynamics (e.g. an AR model) for horizons larger than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Постройте прогноз для тех данных, на которых вы не обучались"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Постройте графики спрогнозированного вами среднего и дисперсии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model selection\n",
    "\n",
    "An important practical problem is the determination of the ARCH order p and the GARCH order q for a particular series. Since GARCH models can be treated as ARMA models for squared residuals, traditional model selection criteria such as the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) can be used for selecting models.\n",
    "\n",
    "For daily returns, if attention is restricted to pure ARCH(p) models it is typically found that large values of p are selected by AIC and BIC. For GARCH(p,q) models, those with p,q ≤ 2 are typically selected by AIC and BIC. Low order GARCH(p,q) models are generally preferred to a high order ARCH(p) for reasons of parsimony and better numerical stability of estimation (high order GARCH(p, q) processes often have many local maxima and minima). For many applications, it is hard to beat the simple GARCH(1,1) model.\n",
    "\n",
    "#### Remark — AIC, BIC\n",
    "When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC.\n",
    "\n",
    "Given a set of candidate models for the data, the preferred model is the one with the minimum AIC (BIC) value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other model specifications\n",
    "\n",
    "Models can also be systematically assembled from the three model components:\n",
    "\n",
    "#### 1. Mean model \n",
    "\n",
    "http://arch.readthedocs.io/en/latest/univariate/mean.html#\n",
    "\n",
    "All ARCH models start by specifying a mean model. Some examples:\n",
    "- Zero mean\n",
    "\n",
    "$$ y_t = \\epsilon_t $$\n",
    "\n",
    "- Constant mean\n",
    "\n",
    "$$ y_t = \\mu + \\epsilon_t $$\n",
    "\n",
    "- Autoregressive model with optional exogenous regressors \n",
    "\n",
    "$$ y_t = \\mu + \\sum_{i=1}^p \\phi_i y_{t-i} + \\gamma^T x_t + \\varepsilon_t $$\n",
    "\n",
    "- Exogenous regressors only\n",
    "\n",
    "#### 2. Vol model — volatility  process\n",
    "\n",
    "http://arch.readthedocs.io/en/latest/univariate/volatility.html\n",
    "\n",
    "A volatility process is added to a mean model to capture time-varying volatility.\n",
    "\n",
    "- Constant volatility process — model has the same variance in all periods\n",
    "- GARCH and related model estimation:\n",
    "    - ARCH(p)\n",
    "    - GARCH(p,q)\n",
    "    - GJR-GARCH(p,o,q)\n",
    "    - AVARCH(p)\n",
    "    - AVGARCH(p,q)\n",
    "    - TARCH(p,o,q)\n",
    "    - Models with arbitrary, pre-specified powers\n",
    "\n",
    "where \n",
    "- p — Order of the symmetric innovation\n",
    "- o – Order of the asymmetric innovation\n",
    "- q – Order of the lagged (transformed) conditional variance\n",
    "\n",
    "#### 3. Distribution of $z_t$\n",
    "http://arch.readthedocs.io/en/latest/univariate/distribution.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задайте модель с произвольным распределением среднего, остатков, количеством лагов\n",
    "# Обучите ее, посмотрите на стандартные остатки, прогноз для среднего и волатильности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate GARCH\n",
    "\n",
    "When hopping from univariate volatility forecasts to multivariate volatility forecast, we need to understand that now we have to forecast not only the univariate volatility element, which we already know how to do, but also the covariance elements. \n",
    "\n",
    "The estimation will be implemented in R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
